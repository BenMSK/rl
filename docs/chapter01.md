---
layout: page
group: "Chapter. 1"
title: "1. Markov Chain (MC)"
---
- Reinforcement Learning (이하 RL) 을 살펴보기 전에 반드시 먼저 이해해야 하는 모델은 바로 MDP (Markov Decision Process)이다.
- 하지만 MDP를 이해하기 위해서는 먼저 MC (Markov Chain) 모델부터 알아야한다.
    - 당연히 MDP에 비해 좀 더 간단한 모델이고 MDP의 기본 베이스가 되는 모델이다.
- 여기서는 아주 간단하게 MC 모델이 무엇인지 정도만 이해하고 넘어가도록 한다.

- - -

- 마코프 체인(이하 MC)은 이산 확률 프로세스(discrete stochastic process)이다.
    - 참고로 연속(continuous) 확률 프로세스를 다루는 MC가 있긴 하다. 하지만 여기서는 다루지 않는다.
- 여기서 확률 프로세스란 확률 분포를 가진 랜덤 변수(random variable)가 일정한 시간 간격(time interval)으로 값을 발생시키는 것을 모델링하는 것이다.
- 이러한 모델 중 현재의 상태가 오로지 이전 상태에만 영향을 받는 확률 프로세스를 MC 프로세스라고 한다.
- MC 모델은 딱 두 가지 속성으로 나타낼 수 있다.
    - \\( X \\) : (유한한) 상태 공간(state space)의 집합.
    - \\( P \\) : 전이 확률 (transition probabilty) 테이블. 모든 상태(state) \\( X \\) 사이의 전이 확률을 의미한다.
    - 이를 아래와 같이 표기한다.

$$MC\;(X, P)$$

- 별로 어렵게 생각할 필요는 없고 주어진 문제를 위의 두 가지 요소로 기술한다고 생각하면 된다.
- 따라서 이 두가지 표현할 수 없는 문제는 당연히 MC로 해결할 수 없다.
- 일단 MC로 처리 가능한 문제에 대해서만 집중하도록 하자.

- - -

- 스텝 (*step*)
    - 각 상태의 전이는 이산 시간(discrete time)에 이루어지며 \\(X\\) 에 속하는 임의의 상태에 머무를 때의 시간을 스텝(step)이라고 한다.
    - 만약 현재 스텝을 \\(n\\) 이라 하면, 이 다음 스텝은 \\(n+1\\) 라고 기술할 수 있다.
- 이 때의 확률 전이 값을 간단하게 기술해보자.
    - \\(p\_{ij}\\) 는 상태 \\(i\\) 에서 \\(j\\) 로 전이될 확률 값을 의미한다.
    - \\(X_n\\) 은 스텝 \\(n\\) 에서 머물러있는 상태(state)를 의미하며 정확히는 해당 상태(state)에 대한 **랜덤 변수** (즉, *r.v.*)를 의미한다.
    
$$p_{ij} = p\left(X_{n+1}=j\;|\;X_n=i\;\right)\qquad\forall\;i, j \in X$$

- 중요한 것은 상태 \\(i\\) 를 방문했을 때, 그 이전에 어떤 상태에 방문했는지 상관 없이 상태 \\(i\\) 에서의 다음 상태 \\(j\\) 로의 전이 확률 값은 언제나 동일하다.
    - 이것은 MC 프로세스의 가정이며 모델을 단순하게 만들어준다.
    - MC의 무기억성 속성이라고도 한다.
    - 수식으로 표현하면 다음과 같다.

$$p_{ij} = p(X_{n+1}\;|\;X_n=i_n , X_{n-1}=i_{n-1},...X_0=i_0) = p(X_{n+1}|X_n=i_n)$$
 
- 이러한 특성(다음 상태의 확률이 오로지 전 상태에만 영향을 받는 특성)을 **마코프 속성(Markovian Property)**이라고 부른다.
- 추가적으로 전이(transition) 확률의 조건은 다음과 같다.

$$p_{ij} \ge 0$$

$$\sum_{j \in X}p_{ij}=1,\qquad\forall i \in X$$

- 이것은 전이 함수가 확률 함수이기 때문에 당연히 성립하는 식이다.

- - -

- 이제 MC 를 그래프로 표현하는 방법을 살펴보도록 하자.
    - 일반적으로 상태(state) \\(X\\) 는 원(circle)으로 표기하고,
    - 전이 확률 \\(P\\) 는 상태 사이의 화살표로 표현하게 된다.
    
![figure1.1]({{ site.baseurl }}/images/ch01_f01.png){:class="center-block" height="80px"}

- 이제 스텝 \\(n\\) 인 상황에서 현재까지 발생한 사건이 발현될 가능성을 확률 값으로 표현해보자.
- 현재 상태에서의 확률 함수는 다음과 같이 나타낼 수 있다.

$$p(X_0=i_0, X_1=i_1,...,X_n=i_n)$$

- 시작 시점부터 현재 상태까지 진행된 결과에 대한 확률값을 표현하고 있다.
- 이를 베이즈 룰을 이용하여 전개한다. 별로 어려운 내용은 없다.

$$p(X_0=i_0, X_1=i_1,...,X_n=i_n) \\=p(X_n=i_n\;|\;X_0=i_0,...,X_{n-1}=i_{n-1})p(X_0=i_0,...,X_{n-1}=i_{n-1})\\=p_{i_0i_1}p_{i_1i_2},...,p_{i_{n-1}i_n}$$

- 이를 *n-step* 전이 확률이라고 한다. 

- - -

- 다음으로 임의의 상태 \\(i\\) 에서 다른 상태 \\(j\\) 로의 *n-step* 전이 확률을 고려해보자.
- 여기서는 상태 \\(i\\) 로부터 \\(n\\) 회 이동을 하여 상태 *j* 에 도달하는 경우를 나타낸다. 이때의 함수를 \\(r\\) 로 표기한다.

$$r_{ij}(n) = p(X_n=j\;|\;X_0=i)$$

- 이를 전개하면,

$$r_{ij}(n) = \sum_{k=1}^K r_{ik}(n-1)p_{kj}$$

- 위와 같이 \\(r\\) 함수에 대해 재귀적으로 표현 가능하고 다시 확률 함수로도 전환 가능하다.

$$r_{ij}(1)= p_{ij}$$

$$r_{ij}(2) = \sum_{k=1}^K r_{ik}(1)p_{kj} = \sum_{k=1}^K p_{ik}p_{kj}=p^{(2)}_{ij}$$

- 위 식에서 \\(p^{2}\\) 에 기술된 지수값 \\(2\\) 는 제곱을 의미하는 것이 아니라 총 *2-step* 임을 나타내는 요약식이다.
    - 즉, 2 단계를 거쳐 \\(i\\) 에서 \\(j\\) 까지 이동하는 확률 값을 의미하게 된다.
    - 이제 이 표현 방법을 일반화하여 아래와 같이 기술할 수 있다.

$$r_{ij}(n)=p^{(n)}_{ij}$$

- 물론 이 식은 RL과 직접적인 관계가 있는 것은 아니지만 개념을 잡기위해 눈여겨두자.

- - -

- 참고로 MC 를 Markov Process (MP) 라고도 표기한다.
    - 보통 MC를 다루는 다른 학문에서는  MC라는 표현을 선호하지만 이상하게 RL 영역에서는 MP표기도 많이 쓰고 있으니 참고하자.
